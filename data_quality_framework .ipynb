{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9e36797e-7be0-41dc-8f97-aad90767a1b2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#---------------------------------------------------------------------------------------------------------\n",
    "# notebook_name : data_quality_framework\n",
    "# notebook_description : this framework notebook used to run a generic data quality checks on a given table\n",
    "# notebook_use : 1) have the below steps as part of a workflow task after the load notebook task (or)\n",
    "#                2) add below command tothe load note book at end of it in a seperate cell\n",
    "#           RunDataQualityNotebookTests(DatabaseName = DatabaseName, TableName = TableName, InsertQuery = Query)\n",
    "#\n",
    "# author : nava\n",
    "# created : 11/24/2021\n",
    "#---------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fda0795b-a54d-477c-a75b-5f793a7b8463",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# notebook parameters\n",
    "dbutils.widgets.text('jobEnv','')\n",
    "dbutils.widgets.text('DatabaseName','')\n",
    "dbutils.widgets.text('TableName','')\n",
    "# dbutils.widgets.text('InsertQuery','')\n",
    "\n",
    "# get notebook parametrs into local variables\n",
    "jobEnv=dbutils.widgets.get('jobEnv')\n",
    "DatabaseName=dbutils.widgets.get('DatabaseName')\n",
    "TableName=dbutils.widgets.get('TableName')\n",
    "\n",
    "#InsertQuery=dbutils.widgets.get('InsertQuery')\n",
    "if jobEnv=='qa':\n",
    "    job_catalog=''\n",
    "elif jobEnv=='prod':\n",
    "    job_catalog=''\n",
    "else:\n",
    "    job_catalog=''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "90ab775b-d6a5-4561-a510-036d23f1a792",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  \n"
     ]
    }
   ],
   "source": [
    "# Define Constants\n",
    "\n",
    "print(jobEnv, DatabaseName, TableName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "467ff89e-99e8-4100-8a0a-e86dc65ca7e3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class DataQualityNotebookTesting:\n",
    "  job_catalog: str\n",
    "  DatabaseName: str\n",
    "  TableName: str\n",
    "  InsertQuery: str\n",
    "\n",
    "\n",
    "def RunDataQualityNotebookTests(job_catalog, DatabaseName, TableName):\n",
    "  \"\"\"\n",
    "  function to run the generic tests on a delta table\n",
    "  DatabaseName, TableName : load DatabaseName, TableName\n",
    "  \"\"\"\n",
    "  \n",
    "  # if WorkspaceID in [WS_ENV_PROD, WS_ENV_QA, WS_ENV_DEV]:\n",
    "  print (f'\\n -- Generic Tests Started for {job_catalog}.{DatabaseName}.{TableName}')\n",
    "\n",
    "  try:\n",
    "    print ('-- -- RefTableConfig_test Tests Started')\n",
    "    RefTableConfig_test(job_catalog, DatabaseName, TableName)\n",
    "    print ('-- -- RefTableConfig_test Tests Completed')\n",
    "  except Exception as Error:\n",
    "    print(Error)\n",
    "    exit()\n",
    "\n",
    "  try:\n",
    "    print ('-- -- NaturalKey_test Tests Started')\n",
    "    NaturalKey_test(job_catalog, DatabaseName, TableName)\n",
    "    print ('-- -- NaturalKey_test Tests Completed')\n",
    "  except Exception as Error:\n",
    "    print(Error)\n",
    "    exit()\n",
    "\n",
    "  try:\n",
    "    print ('-- -- NullOrEmptyStringColumns_test Tests Started')\n",
    "    NullOrEmptyStringColumns_test(job_catalog, DatabaseName, TableName)\n",
    "    print ('-- -- NullOrEmptyStringColumns_test Tests Completed')\n",
    "  except Exception as Error:\n",
    "    print(Error)\n",
    "    exit()\n",
    "\n",
    "  try:\n",
    "    print ('-- -- EmptyString_test Tests Started')\n",
    "    EmptyString_test(job_catalog, DatabaseName, TableName)\n",
    "    print ('-- -- EmptyString_test Tests Completed')\n",
    "  except Exception as Error:\n",
    "    print(Error)\n",
    "    exit()\n",
    "\n",
    "  try:\n",
    "    print ('-- -- LeadingTrailingSpaces_test Tests Started')\n",
    "    LeadingTrailingSpaces_test(job_catalog, DatabaseName, TableName)\n",
    "    print ('-- -- LeadingTrailingSpaces_test Tests Completed')\n",
    "  except Exception as Error:\n",
    "    print(Error)\n",
    "    exit()\n",
    "\n",
    "  try:\n",
    "    print ('-- -- RowCount_test Tests Started')\n",
    "    RowCount_test(job_catalog, DatabaseName, TableName)\n",
    "    print ('-- -- RowCount_test Tests Completed')\n",
    "  except Exception as Error:\n",
    "    print(Error)\n",
    "    exit()\n",
    "\n",
    "  try:\n",
    "    print ('-- -- AuditColumns_test Tests Started')\n",
    "    AuditColumns_test(job_catalog, DatabaseName, TableName)\n",
    "    print ('-- -- AuditColumns_test Tests Completed')\n",
    "  except Exception as Error:\n",
    "    print(Error)\n",
    "    exit()\n",
    "\n",
    "  print (f'\\n -- Generic Tests Completed for {job_catalog}.{DatabaseName}.{TableName}')\n",
    "  \n",
    "  # else:\n",
    "    # print (f'\\n -- Generic Tests Failed for {DatabaseName}.{TableName}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "457b700b-dba7-4f72-90c3-747c96677885",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def Insert_Data_Quality_Monitor(DatabaseName, TableName, DataQualityType, StatusMessage):\n",
    "  \"\"\"\n",
    "  Insert data quality check status into data_quality_monitor table\n",
    "  \"\"\"\n",
    "  try:\n",
    "    #check the configs from data_quality_checks\n",
    "    Query = (f'''\n",
    "      insert into {job_catalog}._utils.data_quality_monitor\n",
    "      select \n",
    "        coalesce(max_id, 0) + 1,\n",
    "        current_date(),\n",
    "        \"{StatusMessage}\" dq_run_status,\n",
    "        c.dq_id,\n",
    "        current_timestamp()\n",
    "      from\n",
    "        {job_catalog}._utils.data_quality_configs r\n",
    "        join {job_catalog}._utils.data_quality_checks c\n",
    "          on r.dq_config_id = c.dq_config_id\n",
    "        left join (select dq_id, max(dq_run_id) max_id from {job_catalog}.data_quality_monitor group by dq_id) m\n",
    "          on c.dq_id = m.dq_id\n",
    "      where \n",
    "        r.database_name = \"{DatabaseName}\"\n",
    "        and r.table_name = \"{TableName}\"\n",
    "        and c.dq_type = \"{DataQualityType}\" --\"RowCount_test\"\n",
    "        and r.is_validate = \"Y\"\n",
    "        and c.is_validate = \"Y\"\n",
    "      ''')\n",
    "    \n",
    "    query_data = spark.sql(Query).collect()\n",
    "    row_count = query_data[0]\n",
    "    \n",
    "    if row_count != 0:\n",
    "      return 'Data Quality Check status inserted into monitor table'\n",
    "    else:\n",
    "      print ('\\n FAIL. Data Quality Check status NOT inserted into monitor table, please review.')\n",
    "      return 'FAIL'\n",
    "    \n",
    "  except Exception as Error:\n",
    "    print(Error)\n",
    "    exit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "afef4c7a-ca24-4279-8c51-c7c496441931",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def RefTableConfig_test(job_catalog, DatabaseName, TableName):\n",
    "  \"\"\"\n",
    "  Check if table is in data_quality_configs\n",
    "  \"\"\"\n",
    "  \n",
    "  try:\n",
    "    #declare variables\n",
    "    DataQualityType = 'RefTableConfig_test'\n",
    "    \n",
    "    #data_quality_configs tables\n",
    "    ref_Query = (f'SELECT table_name FROM {job_catalog}._utils.data_quality_configs where is_validate =\"Y\"')\n",
    "    ref_TableInfo = spark.sql(ref_Query).collect()\n",
    "    ref_Tables = [x.table_name for x in ref_TableInfo]\n",
    "    \n",
    "    if TableName in ref_Tables:\n",
    "\n",
    "      Insert_Data_Quality_Monitor(DatabaseName, TableName, DataQualityType, 'PASS')\n",
    "\n",
    "      return 'PASS'\n",
    "    else: \n",
    "      print('\\n data_quality_configs Availability Test' +\n",
    "            f'\\n FAIL. Table {TableName} is not available in data_quality_configs. Please review.')\n",
    "      \n",
    "      Insert_Data_Quality_Monitor(DatabaseName, TableName, DataQualityType, 'FAIL')\n",
    "\n",
    "      return 'FAIL'\n",
    "  \n",
    "  except Exception as Error:\n",
    "    print(Error)\n",
    "    exit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bf595301-2428-461c-a6dd-75c75b9740f7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def NaturalKey_test(job_catalog, DatabaseName, TableName):\n",
    "  \"\"\"\n",
    "  Function to check natural key violations\n",
    "  \"\"\"\n",
    "\n",
    "  try:\n",
    "    #declare variables\n",
    "    DataQualityType = 'NaturalKey_test'\n",
    "    \n",
    "    #get natural key columns from data_quality_checks\n",
    "    Query = (f'''\n",
    "      select \n",
    "        c.dq_id, c.dq_type, c.dq_rule as ValidateColumns \n",
    "      from\n",
    "        {job_catalog}._utils.data_quality_configs r\n",
    "        join {job_catalog}._utils.data_quality_checks c\n",
    "          on r.dq_config_id = c.dq_config_id\n",
    "      where \n",
    "        r.database_name = \"{DatabaseName}\"\n",
    "        and r.table_name = \"{TableName}\"\n",
    "        and c.dq_type = \"NaturalKey_test\"\n",
    "        and c.is_validate = \"Y\"\n",
    "      ''')\n",
    "    query_data = spark.sql(Query).collect()\n",
    "    if query_data == []:\n",
    "      #no data for specified table\n",
    "      print('\\n Natural Key Test' +\n",
    "            '\\n FAIL. Natural key info is missing in config table. Please review.')\n",
    "      Insert_Data_Quality_Monitor(DatabaseName, TableName, DataQualityType, 'FAIL')\n",
    "      return 'FAIL'\n",
    "    NK_str = (query_data[0].ValidateColumns)\n",
    "    \n",
    "    if NK_str == '':\n",
    "      #no NK data\n",
    "      print('\\n Natural Key Test' +\n",
    "            '\\n FAIL. No natural key provided. Please review.')\n",
    "      Insert_Data_Quality_Monitor(DatabaseName, TableName, DataQualityType, 'FAIL')\n",
    "      return 'FAIL'\n",
    "    \n",
    "    #check NK violation\n",
    "    query_count_NK = (f'SELECT {NK_str} FROM {job_catalog}.{DatabaseName}.{TableName} GROUP BY {NK_str} HAVING COUNT(*) > 1')\n",
    "    DF_qc_NK = spark.sql(query_count_NK).collect()\n",
    "\n",
    "    if len(DF_qc_NK) > 0:\n",
    "      print('\\n Natural Key Test' +\n",
    "            '\\n Query used for testing:'+\n",
    "            f'\\n {query_count_NK}'\n",
    "            '\\n FAIL. Natural Key Violation! Please review.')\n",
    "      Insert_Data_Quality_Monitor(DatabaseName, TableName, DataQualityType, 'FAIL')\n",
    "      return 'FAIL'\n",
    "    else:\n",
    "      Insert_Data_Quality_Monitor(DatabaseName, TableName, DataQualityType, 'PASS')\n",
    "      return 'PASS'\n",
    "  \n",
    "  except Exception as Error:\n",
    "    print(Error)\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "968628eb-a093-4cc8-83ce-e3ceda4ec1fe",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def NullOrEmptyStringColumns_test(job_catalog, DatabaseName, TableName):\n",
    "  \"\"\"Check for columns with only NULL or empty strings\"\"\"\n",
    "\n",
    "  try:\n",
    "    #declare variables\n",
    "    DataQualityType = 'NullOrEmptyStringColumns_test'\n",
    "\n",
    "    #check the configs from data_quality_checks\n",
    "    Query = (f'''\n",
    "      select \n",
    "        c.dq_id, c.dq_type, c.dq_rule as ValidateColumns \n",
    "      from\n",
    "        {job_catalog}._utils.data_quality_configs r\n",
    "        join {job_catalog}._utils.data_quality_checks c\n",
    "          on r.dq_config_id = c.dq_config_id\n",
    "      where \n",
    "        r.database_name = \"{DatabaseName}\"\n",
    "        and r.table_name = \"{TableName}\"\n",
    "        and c.dq_type = \"NullOrEmptyStringColumns_test\"\n",
    "        and c.is_validate = \"Y\"\n",
    "      ''')\n",
    "    \n",
    "    query_data = spark.sql(Query).collect()\n",
    "    \n",
    "    if query_data == []:\n",
    "      #no data for specified table\n",
    "      print('\\n Null or Empty String Columns Test' +\n",
    "            '\\n FAIL. NullOrEmptyStringColumns_test config data is missing. Please review.')\n",
    "      Insert_Data_Quality_Monitor(DatabaseName, TableName, DataQualityType, 'FAIL')\n",
    "      return 'FAIL'\n",
    "    \n",
    "    #get table row count\n",
    "    table_count_query = (f'select count(1) as CNT FROM {job_catalog}.{DatabaseName}.{TableName}')\n",
    "    count_data = spark.sql(table_count_query).collect()\n",
    "    table_len = count_data[0].CNT\n",
    "    \n",
    "    #get table columns, remove standard columns\n",
    "    if (len(query_data) == 1 and query_data[0][2] == 'all_columns'):\n",
    "      TableCols = [column.name for column in spark.catalog.listColumns(TableName,DatabaseName)]\n",
    "      for val in ['CreateUser','CreateDate','UpdateUser','UpdateDate']:\n",
    "        TableCols.remove(val)\n",
    "    else:\n",
    "      TableCols = [i for i in query_data[0][2].split(',')]\n",
    "      \n",
    "     \n",
    "    print(TableCols)\n",
    "\n",
    "    #potential improvement. Auto create 1 query with all column counts via union statement.\n",
    "    null_cols = []\n",
    "    for column in TableCols:\n",
    "      \n",
    "      \n",
    "      Query = (f'''\n",
    "        SELECT {table_len} - \n",
    "        (\n",
    "          SELECT \n",
    "            COUNT(1) FROM {job_catalog}.{DatabaseName}.{TableName} \n",
    "           WHERE \n",
    "             {column} IS NULL OR LTRIM(RTRIM({column})) = \"\"\n",
    "         ) AS RecordCnt\n",
    "        ''')\n",
    "      \n",
    "      record_count_data = spark.sql(Query).collect()\n",
    "      record_count = record_count_data[0].RecordCnt\n",
    "\n",
    "      if record_count == 0:\n",
    "        null_cols.append(column)\n",
    "\n",
    "    if null_cols == []:\n",
    "      Insert_Data_Quality_Monitor(DatabaseName, TableName, DataQualityType, 'PASS')\n",
    "      return 'PASS'\n",
    "    else:\n",
    "      null_col_str = ', '.join(null_cols)\n",
    "      print('\\n Null or Empty String Test' +\n",
    "            f'\\n FAIL. Columns {null_col_str} have only NULL or empty string values. Please review')\n",
    "      Insert_Data_Quality_Monitor(DatabaseName, TableName, DataQualityType, 'FAIL')\n",
    "      return 'FAIL'\n",
    "  \n",
    "  except Exception as Error:\n",
    "    print(Error)\n",
    "    exit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9f43d6a8-0373-4ef9-ba47-b4bcc8588dd9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def EmptyString_test(job_catalog, DatabaseName, TableName):\n",
    "  \"\"\"\n",
    "  Check for columns with empty string records\n",
    "  \"\"\"\n",
    "  try:\n",
    "    #declare variables\n",
    "    DataQualityType = 'EmptyString_test'\n",
    "\n",
    "    #check the configs from data_quality_checks\n",
    "    Query = (f'''\n",
    "      select \n",
    "        c.dq_id, c.dq_type, c.dq_rule as ValidateColumns \n",
    "      from\n",
    "        {job_catalog}._utils.data_quality_configs r\n",
    "        join {job_catalog}._utils.data_quality_checks c\n",
    "          on r.dq_config_id = c.dq_config_id\n",
    "      where \n",
    "        r.database_name = \"{DatabaseName}\"\n",
    "        and r.table_name = \"{TableName}\"\n",
    "        and c.dq_type = \"EmptyString_test\"\n",
    "        and c.is_validate = \"Y\"\n",
    "      ''')\n",
    "    query_data = spark.sql(Query).collect()\n",
    "    if query_data == []:\n",
    "      #no data for specified table\n",
    "      print('\\n Empty String Columns Test' +\n",
    "            '\\n FAIL. EmptyString_test config data is missing. Please review.')\n",
    "      Insert_Data_Quality_Monitor(DatabaseName, TableName, DataQualityType, 'FAIL')\n",
    "      return 'FAIL'\n",
    "\n",
    "    #get table columns, remove standard columns\n",
    "    if (len(query_data) == 1 and query_data[0][2] == 'all_columns'):\n",
    "      TableCols = [column.name for column in spark.catalog.listColumns(TableName,DatabaseName)]\n",
    "      for val in ['CreateUser','CreateDate','UpdateUser','UpdateDate']: # do not include audit columns\n",
    "        TableCols.remove(val)\n",
    "    else:\n",
    "      TableCols = [i for i in query_data[0][2].split(',')]\n",
    "      \n",
    "    print(TableCols)\n",
    "    \n",
    "    empty_string_cols = []\n",
    "    for column in TableCols:\n",
    "      Query = f'SELECT COUNT(1) AS RecordCnt FROM {job_catalog}.{DatabaseName}.{TableName} WHERE LTRIM(RTRIM({column})) = \"\"'\n",
    "      \n",
    "      record_count_data = spark.sql(Query).collect()\n",
    "      record_count = record_count_data[0].RecordCnt\n",
    "\n",
    "      if record_count > 0:\n",
    "        empty_string_cols.append(column)\n",
    "      \n",
    "    if empty_string_cols == []:\n",
    "      Insert_Data_Quality_Monitor(DatabaseName, TableName, DataQualityType, 'PASS')\n",
    "      return 'PASS'\n",
    "    else:\n",
    "      empty_string_cols_str = ', '.join(empty_string_cols)\n",
    "      print('\\n Empty String Test' +\n",
    "            f'\\n FAIL. Column(s) {empty_string_cols_str} have empty string values. Please review. ' +\n",
    "            '\\n Consider using RunEmptyStringToNull function from Import_class_library')\n",
    "      Insert_Data_Quality_Monitor(DatabaseName, TableName, DataQualityType, 'FAIL')\n",
    "      return 'FAIL'\n",
    "  \n",
    "  except Exception as Error:\n",
    "    print(Error)\n",
    "    exit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "08ab9ad6-b394-44cc-9011-99ab8177da52",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def LeadingTrailingSpaces_test(job_catalog, DatabaseName, TableName):\n",
    "  \"\"\"\n",
    "  Check for columns with leading/trailing spaces\n",
    "  \"\"\"\n",
    "  try:\n",
    "    #declare variables\n",
    "    DataQualityType = 'LeadingTrailingSpaces_test'\n",
    "\n",
    "    #check the configs from data_quality_checks\n",
    "    Query = (f'''\n",
    "      select \n",
    "        c.dq_id, c.dq_type, c.dq_rule as ValidateColumns \n",
    "      from\n",
    "        {job_catalog}._utils.data_quality_configs r\n",
    "        join {job_catalog}._utils.data_quality_checks c\n",
    "          on r.dq_config_id = c.dq_config_id\n",
    "      where \n",
    "        r.database_name = \"{DatabaseName}\"\n",
    "        and r.table_name = \"{TableName}\"\n",
    "        and c.dq_type = \"LeadingTrailingSpaces_test\"\n",
    "        and c.is_validate = \"Y\"\n",
    "      ''')\n",
    "    query_data = spark.sql(Query).collect()\n",
    "    if query_data == []:\n",
    "      #no data for specified table\n",
    "      print('\\n leading trailing spaces Test' +\n",
    "            '\\n FAIL. LeadingTrailingSpaces_test config data is missing. Please review.')\n",
    "      Insert_Data_Quality_Monitor(DatabaseName, TableName, DataQualityType, 'FAIL')\n",
    "      return 'FAIL'\n",
    "\n",
    "    #get table columns, remove standard columns\n",
    "    if (len(query_data) == 1 and query_data[0][2] == 'all_columns'):\n",
    "      TableCols = [column.name for column in spark.catalog.listColumns(TableName,DatabaseName)]\n",
    "      for val in ['CreateUser','CreateDate','UpdateUser','UpdateDate']: # do not include audit columns\n",
    "        TableCols.remove(val)\n",
    "    else:\n",
    "      TableCols = [i for i in query_data[0][2].split(',')]\n",
    "      \n",
    "    print(TableCols)\n",
    "    \n",
    "    #Build testing query\n",
    "    Query = (f\"\"\"SELECT count(1) as CNT FROM {job_catalog}.{DatabaseName}.{TableName} WHERE\"\"\")\n",
    "    for x in range (len(TableCols)):\n",
    "      col = TableCols[x]\n",
    "      if x==0:\n",
    "        Query += f\"\\n(left({col},1) = ' ' OR right({col},1) = ' ')\"\n",
    "      else:\n",
    "        Query += f\"\\nOR (left({col},1) = ' ' OR right({col},1) = ' ')\"\n",
    "    \n",
    "    #Collect query data\n",
    "    query_data = spark.sql(Query).collect()\n",
    "    row_count = query_data[0]['CNT']\n",
    "        \n",
    "    if row_count == 0:\n",
    "      Insert_Data_Quality_Monitor(DatabaseName, TableName, DataQualityType, 'PASS')\n",
    "      return 'PASS'\n",
    "    else:\n",
    "      print('\\n Leading/Trailing Spaces Test' +\n",
    "            '\\n FAIL. There are column(s) that have leading/trailing spaces. Please review.' +\n",
    "            '\\n Consider using RunTrimColumns function from Import_class_library')\n",
    "      Insert_Data_Quality_Monitor(DatabaseName, TableName, DataQualityType, 'FAIL')\n",
    "      return 'FAIL'\n",
    "    \n",
    "  except Exception as Error:\n",
    "    print(Error)\n",
    "    exit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "112069eb-495a-40e6-ac73-8e0d1cbb1ab2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def RowCount_test(job_catalog, DatabaseName, TableName):\n",
    "  \"\"\"\n",
    "  Check row count\n",
    "  \"\"\"\n",
    "  try:\n",
    "    #declare variables\n",
    "    DataQualityType = 'RowCount_test'\n",
    "\n",
    "    #check the configs from data_quality_checks\n",
    "    Query = (f'''\n",
    "      select \n",
    "        c.dq_id, c.dq_type, c.dq_rule as ValidateColumns \n",
    "      from\n",
    "        {job_catalog}._utils.data_quality_configs r\n",
    "        join {job_catalog}._utils.data_quality_checks c\n",
    "          on r.dq_config_id = c.dq_config_id\n",
    "      where \n",
    "        r.database_name = \"{DatabaseName}\"\n",
    "        and r.table_name = \"{TableName}\"\n",
    "        and c.dq_type = \"RowCount_test\"\n",
    "        and c.is_validate = \"Y\"\n",
    "      ''')\n",
    "    query_data = spark.sql(Query).collect()\n",
    "    if query_data == []:\n",
    "      #no data for specified table\n",
    "      print('\\n table row count Test' +\n",
    "            '\\n FAIL. RowCount_test config data is missing. Please review.')\n",
    "      Insert_Data_Quality_Monitor(DatabaseName, TableName, DataQualityType, 'FAIL')\n",
    "      return 'FAIL'\n",
    "\n",
    "    #get data\n",
    "    Query = (f'select count(1) as CNT from {job_catalog}.{DatabaseName}.{TableName}')\n",
    "    query_data = spark.sql(Query).collect()\n",
    "    \n",
    "    row_count = query_data[0]['CNT']\n",
    "    \n",
    "    if row_count != 0:\n",
    "      Insert_Data_Quality_Monitor(DatabaseName, TableName, DataQualityType, 'PASS')\n",
    "      return 'PASS'\n",
    "    else:\n",
    "      print ('\\n Row Count Test' +\n",
    "             '\\n FAIL. The result table has 0 rows. Please review.')\n",
    "      Insert_Data_Quality_Monitor(DatabaseName, TableName, DataQualityType, 'FAIL')\n",
    "      return 'FAIL'\n",
    "    \n",
    "  except Exception as Error:\n",
    "    print(Error)\n",
    "    exit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "62ff23ac-d1ff-4288-b808-74a6b1bc8b7c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def AuditColumns_test(job_catalog, DatabaseName, TableName):\n",
    "  \"\"\"\n",
    "  Check audit columns existence\n",
    "  \"\"\"\n",
    "  try:\n",
    "    #declare variables\n",
    "    DataQualityType = 'AuditColumns_test'\n",
    "\n",
    "    #check the configs from data_quality_checks\n",
    "    Query = (f'''\n",
    "      select \n",
    "        c.dq_id, c.dq_type, c.dq_rule as ValidateColumns \n",
    "      from\n",
    "        {job_catalog}._utils.data_quality_configs r\n",
    "        join {job_catalog}._utils.data_quality_checks c\n",
    "          on r.dq_config_id = c.dq_config_id\n",
    "      where \n",
    "        r.database_name = \"{DatabaseName}\"\n",
    "        and r.table_name = \"{TableName}\"\n",
    "        and c.dq_type = \"AuditColumns_test\"\n",
    "        and c.is_validate = \"Y\"\n",
    "      ''')\n",
    "    query_data = spark.sql(Query).collect()\n",
    "    if query_data == []:\n",
    "      #no data for specified table\n",
    "      print('\\n audit columns Test' +\n",
    "            '\\n FAIL. AuditColumns_test config data is missing. Please review.')\n",
    "      Insert_Data_Quality_Monitor(DatabaseName, TableName, DataQualityType, 'FAIL')\n",
    "      return 'FAIL'\n",
    "\n",
    "    dq_columns = [i for i in query_data[0][2].split(',')]\n",
    "    print(dq_columns)\n",
    "\n",
    "    audit_columns = []\n",
    "    for col in dq_columns:\n",
    "        Query = (f'''SELECT distinct column_name FROM system.information_schema.columns where table_schema = \"{DatabaseName}\" AND table_name = \"{TableName}\" AND column_name = \"{col}\"''')\n",
    "\n",
    "        query_data = spark.sql(Query).collect()\n",
    "        audit_columns.append(query_data[0][0])\n",
    "    print(audit_columns)\n",
    "    \n",
    "    #check existence of audit columns\n",
    "    missing_cols = []\n",
    "    for val in dq_columns:\n",
    "      if val in audit_columns:\n",
    "        pass\n",
    "      else:\n",
    "        missing_cols.append(val)\n",
    "\n",
    "    if missing_cols == []:\n",
    "      Insert_Data_Quality_Monitor(DatabaseName, TableName, DataQualityType, 'PASS')\n",
    "      return 'PASS'\n",
    "    else:\n",
    "      missing_cols_str = ', '.join(missing_cols)\n",
    "      print('\\n Audit Columns Test' +\n",
    "            f'\\n FAIL. The following audit column(s) are missing: {missing_cols_str}.' +\n",
    "            '\\n Please add column(s) and review.')\n",
    "      Insert_Data_Quality_Monitor(DatabaseName, TableName, DataQualityType, 'FAIL')\n",
    "      return 'FAIL'\n",
    "    \n",
    "  except Exception as Error:\n",
    "    print(Error)\n",
    "    exit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2af83bc0-ed4b-4ff1-aebd-fa8e2a97f362",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " -- Generic Tests Started for .\n",
      "-- -- RefTableConfig_test Tests Started\n",
      "\n",
      " data_quality_configs Availability Test\n",
      " FAIL. Table  is not available in data_quality_configs. Please review.\n",
      "-- -- RefTableConfig_test Tests Completed\n",
      "-- -- NaturalKey_test Tests Started\n",
      "\n",
      " Natural Key Test\n",
      " FAIL. Natural key info is missing in config table. Please review.\n",
      "-- -- NaturalKey_test Tests Completed\n",
      "-- -- NullOrEmptyStringColumns_test Tests Started\n",
      "\n",
      " Null or Empty String Columns Test\n",
      " FAIL. NullOrEmptyStringColumns_test config data is missing. Please review.\n",
      "-- -- NullOrEmptyStringColumns_test Tests Completed\n",
      "-- -- EmptyString_test Tests Started\n",
      "\n",
      " Empty String Columns Test\n",
      " FAIL. EmptyString_test config data is missing. Please review.\n",
      "-- -- EmptyString_test Tests Completed\n",
      "-- -- LeadingTrailingSpaces_test Tests Started\n",
      "\n",
      " leading trailing spaces Test\n",
      " FAIL. LeadingTrailingSpaces_test config data is missing. Please review.\n",
      "-- -- LeadingTrailingSpaces_test Tests Completed\n",
      "-- -- RowCount_test Tests Started\n",
      "\n",
      " table row count Test\n",
      " FAIL. RowCount_test config data is missing. Please review.\n",
      "-- -- RowCount_test Tests Completed\n",
      "-- -- AuditColumns_test Tests Started\n",
      "\n",
      " audit columns Test\n",
      " FAIL. AuditColumns_test config data is missing. Please review.\n",
      "-- -- AuditColumns_test Tests Completed\n",
      "\n",
      " -- Generic Tests Completed for .\n"
     ]
    }
   ],
   "source": [
    "RunDataQualityNotebookTests(job_catalog, DatabaseName, TableName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c536a100-477a-4512-9846-e5fdcce5debc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.notebook.exit(\"PASS\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "data_quality_framework",
   "widgets": {
    "DatabaseName": {
     "currentValue": "",
     "nuid": "cc22361f-4d61-4410-9087-775557d2222f",
     "widgetInfo": {
      "defaultValue": "",
      "label": null,
      "name": "DatabaseName",
      "options": {
       "autoCreated": null,
       "validationRegex": null,
       "widgetType": "text"
      },
      "widgetType": "text"
     }
    },
    "TableName": {
     "currentValue": "",
     "nuid": "789095d6-a34c-447a-addc-8b4d094aeed1",
     "widgetInfo": {
      "defaultValue": "",
      "label": null,
      "name": "TableName",
      "options": {
       "autoCreated": null,
       "validationRegex": null,
       "widgetType": "text"
      },
      "widgetType": "text"
     }
    },
    "jobEnv": {
     "currentValue": "",
     "nuid": "5c5631bc-5973-4667-88fb-b6dd581fed10",
     "widgetInfo": {
      "defaultValue": "",
      "label": null,
      "name": "jobEnv",
      "options": {
       "autoCreated": null,
       "validationRegex": null,
       "widgetType": "text"
      },
      "widgetType": "text"
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
